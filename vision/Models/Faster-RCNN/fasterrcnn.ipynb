{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10642250,"sourceType":"datasetVersion","datasetId":6589303}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport json\n\n\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom torchvision import transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchmetrics.detection import MeanAveragePrecision\n\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:48:42.053041Z","iopub.execute_input":"2025-02-21T22:48:42.053408Z","iopub.status.idle":"2025-02-21T22:48:42.058708Z","shell.execute_reply.started":"2025-02-21T22:48:42.053382Z","shell.execute_reply":"2025-02-21T22:48:42.057773Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"transform = T.Compose([\n    T.ToTensor(),  \n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:39:44.213018Z","iopub.execute_input":"2025-02-21T22:39:44.213237Z","iopub.status.idle":"2025-02-21T22:39:44.229677Z","shell.execute_reply.started":"2025-02-21T22:39:44.213217Z","shell.execute_reply":"2025-02-21T22:39:44.228776Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, img_dir, annotation_path, transforms=None):\n        self.img_dir = img_dir\n        self.transforms = transforms\n\n        with open(annotation_path, \"r\") as f:\n            self.annotations = json.load(f)\n\n        self.image_info = {img[\"id\"]: img for img in self.annotations[\"images\"]}\n\n        self.img_to_anns = {}\n        for ann in self.annotations[\"annotations\"]:\n            img_id = ann[\"image_id\"]\n            if img_id not in self.img_to_anns:\n                self.img_to_anns[img_id] = []\n            self.img_to_anns[img_id].append(ann)\n\n        self.img_ids = list(self.image_info.keys())\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n\n        img_info = self.image_info[img_id]\n        img_name = img_info[\"file_name\"]\n        img_path = os.path.join(self.img_dir, img_name)\n\n        img = Image.open(img_path).convert(\"RGB\")\n\n        anns = self.img_to_anns.get(img_id, [])\n\n        boxes = []\n        labels = []\n        for ann in anns:\n            bbox = ann[\"bbox\"]\n            x_min, y_min, width, height = bbox\n            x_max = x_min + width\n            y_max = y_min + height\n            boxes.append([x_min, y_min, x_max, y_max])\n            labels.append(ann[\"category_id\"])\n\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:39:44.230944Z","iopub.execute_input":"2025-02-21T22:39:44.231156Z","iopub.status.idle":"2025-02-21T22:39:44.241479Z","shell.execute_reply.started":"2025-02-21T22:39:44.231136Z","shell.execute_reply":"2025-02-21T22:39:44.240398Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"img_dir = \"/kaggle/input/tomatodiseasecocogp/train\"\nannotation_path = \"/kaggle/input/tomatodiseasecocogp/train/_annotations.coco.json\"\ntrain_dataset = CustomDataset(img_dir, annotation_path, transforms=transform)\n\nvalidation_dataset = CustomDataset('/kaggle/input/tomatodiseasecocogp/valid', '/kaggle/input/tomatodiseasecocogp/valid/_annotations.coco.json', transforms=transform)\ntest_dataset = CustomDataset('/kaggle/input/tomatodiseasecocogp/test', '/kaggle/input/tomatodiseasecocogp/test/_annotations.coco.json', transforms=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:39:44.242329Z","iopub.execute_input":"2025-02-21T22:39:44.242845Z","iopub.status.idle":"2025-02-21T22:39:44.723609Z","shell.execute_reply.started":"2025-02-21T22:39:44.242807Z","shell.execute_reply":"2025-02-21T22:39:44.722166Z"}},"outputs":[{"name":"stdout","text":"Image shape: torch.Size([3, 512, 512])\nBoxes: tensor([[254., 106., 338., 221.],\n        [261., 233., 332., 401.],\n        [163.,  48., 256., 192.],\n        [357., 172., 466., 364.],\n        [ 71., 284., 153., 465.],\n        [  7., 105.,  65., 181.],\n        [ 21., 266.,  65., 336.],\n        [ 46., 238.,  90., 268.],\n        [ 71., 212., 129., 276.],\n        [328., 216., 367., 276.],\n        [319., 155., 356., 211.],\n        [190., 175., 233., 227.]])\nLabels: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\nImage shape: torch.Size([3, 512, 512])\nBoxes: tensor([[  0.,   0., 428., 413.]])\nLabels: tensor([4])\nImage shape: torch.Size([3, 512, 512])\nBoxes: tensor([[ 17.0000,   0.0000, 240.0000, 240.0000],\n        [296.0000,  23.0000, 409.0000, 256.0000],\n        [ 15.0000, 256.0000, 237.5000, 506.0000],\n        [286.0000, 271.0000, 456.5000, 450.5000]])\nLabels: tensor([1, 4, 5, 5])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print('Training Data Try')\nimg, target = train_dataset[0]\nprint(\"Image shape:\", img.shape)\nprint(\"Boxes:\", target[\"boxes\"])\nprint(\"Labels:\", target[\"labels\"])\nprint('---------------------------------------------------------------------------')\nprint('Validation Data Try')\nimg, target = validation_dataset[0]\nprint(\"Image shape:\", img.shape)\nprint(\"Boxes:\", target[\"boxes\"])\nprint(\"Labels:\", target[\"labels\"])\nprint('---------------------------------------------------------------------------')\nprint('Test Data Try')\nimg, target = test_dataset[0]\nprint(\"Image shape:\", img.shape)\nprint(\"Boxes:\", target[\"boxes\"])\nprint(\"Labels:\", target[\"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:45:34.325384Z","iopub.execute_input":"2025-02-21T22:45:34.325894Z","iopub.status.idle":"2025-02-21T22:45:34.357969Z","shell.execute_reply.started":"2025-02-21T22:45:34.325862Z","shell.execute_reply":"2025-02-21T22:45:34.356809Z"}},"outputs":[{"name":"stdout","text":"Training Data Try\nImage shape: torch.Size([3, 512, 512])\nBoxes: tensor([[254., 106., 338., 221.],\n        [261., 233., 332., 401.],\n        [163.,  48., 256., 192.],\n        [357., 172., 466., 364.],\n        [ 71., 284., 153., 465.],\n        [  7., 105.,  65., 181.],\n        [ 21., 266.,  65., 336.],\n        [ 46., 238.,  90., 268.],\n        [ 71., 212., 129., 276.],\n        [328., 216., 367., 276.],\n        [319., 155., 356., 211.],\n        [190., 175., 233., 227.]])\nLabels: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n---------------------------------------------------------------------------\nValidation Data Try\nImage shape: torch.Size([3, 512, 512])\nBoxes: tensor([[  0.,   0., 428., 413.]])\nLabels: tensor([4])\n---------------------------------------------------------------------------\nTest Data Try\nImage shape: torch.Size([3, 512, 512])\nBoxes: tensor([[ 17.0000,   0.0000, 240.0000, 240.0000],\n        [296.0000,  23.0000, 409.0000, 256.0000],\n        [ 15.0000, 256.0000, 237.5000, 506.0000],\n        [286.0000, 271.0000, 456.5000, 450.5000]])\nLabels: tensor([1, 4, 5, 5])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nvalidation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:39:44.724958Z","iopub.execute_input":"2025-02-21T22:39:44.725266Z","iopub.status.idle":"2025-02-21T22:39:44.731710Z","shell.execute_reply.started":"2025-02-21T22:39:44.725235Z","shell.execute_reply":"2025-02-21T22:39:44.730133Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\nnum_classes = 12  \nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:48:48.782937Z","iopub.execute_input":"2025-02-21T22:48:48.783299Z","iopub.status.idle":"2025-02-21T22:48:49.437555Z","shell.execute_reply.started":"2025-02-21T22:48:48.783270Z","shell.execute_reply":"2025-02-21T22:48:49.435959Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  \ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:48:58.113167Z","iopub.execute_input":"2025-02-21T22:48:58.113568Z","iopub.status.idle":"2025-02-21T22:48:58.124307Z","shell.execute_reply.started":"2025-02-21T22:48:58.113536Z","shell.execute_reply":"2025-02-21T22:48:58.123176Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=12, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=48, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:49:06.873729Z","iopub.execute_input":"2025-02-21T22:49:06.874092Z","iopub.status.idle":"2025-02-21T22:49:06.887551Z","shell.execute_reply.started":"2025-02-21T22:49:06.874063Z","shell.execute_reply":"2025-02-21T22:49:06.886659Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"num_epochs = 10\ntrain_losses = {'box': [], 'cls': []}\nval_losses = {'box': [], 'cls': []}\ntrain_metrics = {'precision': [], 'recall': []}\nval_metrics = {'precision': [], 'recall': [], 'mAP50': [], 'mAP50_95': []}\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss_box, train_loss_cls = 0.0, 0.0\n    all_preds_train, all_targets_train = [], []\n\n    for images, targets in train_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        train_loss_box += loss_dict.get('loss_box_reg', 0.0).item()\n        train_loss_cls += loss_dict.get('loss_classifier', 0.0).item()\n\n        losses.backward()\n        optimizer.step()\n\n    train_loss_box /= len(train_loader)\n    train_loss_cls /= len(train_loader)\n    train_losses['box'].append(train_loss_box)\n    train_losses['cls'].append(train_loss_cls)\n\n    model.eval()\n    val_loss_box, val_loss_cls = 0.0, 0.0\n    all_preds_val, all_targets_val = [], []\n\n    with torch.no_grad():\n        for images, targets in validation_loader:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            val_loss_box += loss_dict.get('loss_box_reg', 0.0).item()\n            val_loss_cls += loss_dict.get('loss_classifier', 0.0).item()\n\n            outputs = model(images)\n            for output, target in zip(outputs, targets):\n                pred_boxes = output['boxes'].cpu().numpy()\n                pred_labels = output['labels'].cpu().numpy()\n                true_labels = target['labels'].cpu().numpy()\n                all_preds_val.extend(pred_labels)\n                all_targets_val.extend(true_labels)\n\n    val_loss_box /= len(validation_loader)\n    val_loss_cls /= len(validation_loader)\n    val_losses['box'].append(val_loss_box)\n    val_losses['cls'].append(val_loss_cls)\n\n    val_cm = confusion_matrix(all_targets_val, all_preds_val)\n    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_targets_val, all_preds_val, average='weighted')\n\n    map_metric = MeanAveragePrecision(iou_type=\"bbox\")\n    preds_val = [{'boxes': torch.tensor(o['boxes']), 'scores': torch.tensor(o['scores']), 'labels': torch.tensor(o['labels'])} for o in outputs]\n    targets_val = [{'boxes': t['boxes'], 'labels': t['labels']} for t in targets]\n    map_metric.update(preds_val, targets_val)\n    map_results = map_metric.compute()\n    val_metrics['precision'].append(val_precision)\n    val_metrics['recall'].append(val_recall)\n    val_metrics['mAP50'].append(map_results['map_50'].item())\n    val_metrics['mAP50_95'].append(map_results['map'].item())\n\n    lr_scheduler.step()\n\n    print(f'Epoch: {epoch + 1}, Train Loss (Box/Cls): {train_loss_box:.4f}/{train_loss_cls:.4f}, '\n          f'Val Loss (Box/Cls): {val_loss_box:.4f}/{val_loss_cls:.4f}, '\n          f'Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val mAP50: {map_results[\"map_50\"].item():.4f}, Val mAP50:95: {map_results[\"map\"].item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:51:37.784179Z","iopub.execute_input":"2025-02-21T22:51:37.784618Z","execution_failed":"2025-02-21T22:57:03.144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting results\nplt.figure(figsize=(15, 10))\n\n# Train Losses\nplt.subplot(3, 4, 1)\nplt.plot(train_losses['box'], label='train/box_loss')\nplt.plot([x * 0.9 for x in train_losses['box']], '--', label='smooth')\nplt.title('train/box_loss')\nplt.legend()\n\nplt.subplot(3, 4, 2)\nplt.plot(train_losses['cls'], label='train/cls_loss')\nplt.plot([x * 0.9 for x in train_losses['cls']], '--', label='smooth')\nplt.title('train/cls_loss')\nplt.legend()\n\n# Validation Losses\nplt.subplot(3, 4, 5)\nplt.plot(val_losses['box'], label='val/box_loss')\nplt.plot([x * 0.9 for x in val_losses['box']], '--', label='smooth')\nplt.title('val/box_loss')\nplt.legend()\n\nplt.subplot(3, 4, 6)\nplt.plot(val_losses['cls'], label='val/cls_loss')\nplt.plot([x * 0.9 for x in val_losses['cls']], '--', label='smooth')\nplt.title('val/cls_loss')\nplt.legend()\n\n# Metrics\nplt.subplot(3, 4, 9)\nplt.plot(train_metrics['precision'], label='metrics/precision(B)')\nplt.plot([x * 0.95 for x in train_metrics['precision']], '--', label='smooth')\nplt.title('metrics/precision(B)')\nplt.legend()\n\nplt.subplot(3, 4, 10)\nplt.plot(train_metrics['recall'], label='metrics/recall(B)')\nplt.plot([x * 0.95 for x in train_metrics['recall']], '--', label='smooth')\nplt.title('metrics/recall(B)')\nplt.legend()\n\nplt.subplot(3, 4, 11)\nplt.plot(val_metrics['mAP50'], label='metrics/mAP50(B)')\nplt.plot([x * 0.95 for x in val_metrics['mAP50']], '--', label='smooth')\nplt.title('metrics/mAP50(B)')\nplt.legend()\n\nplt.subplot(3, 4, 12)\nplt.plot(val_metrics['mAP50_95'], label='metrics/mAP50-95(B)')\nplt.plot([x * 0.95 for x in val_metrics['mAP50_95']], '--', label='smooth')\nplt.title('metrics/mAP50-95(B)')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('training_results.png')\nplt.close()\n\n# Confusion Matrix for Validation\nval_cm_normalized = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nplt.imshow(val_cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Validation Confusion Matrix')\nplt.colorbar()\nplt.savefig('val_confusion_matrix.png')\nplt.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:40:44.082929Z","iopub.status.idle":"2025-02-21T22:40:44.083451Z","shell.execute_reply.started":"2025-02-21T22:40:44.083134Z","shell.execute_reply":"2025-02-21T22:40:44.083178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion Matrix for Test \nmodel.eval()\nall_preds_test, all_targets_test = [], []\nwith torch.no_grad():\n    for images, targets in test_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        outputs = model(images)\n        for output, target in zip(outputs, targets):\n            pred_labels = output['labels'].cpu().numpy()\n            true_labels = target['labels'].cpu().numpy()\n            all_preds_test.extend(pred_labels)\n            all_targets_test.extend(true_labels)\n\ntest_cm = confusion_matrix(all_targets_test, all_preds_test)\ntest_cm_normalized = test_cm.astype('float') / test_cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nplt.imshow(test_cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Test Confusion Matrix')\nplt.colorbar()\nplt.savefig('test_confusion_matrix.png')\nplt.close()\n\n# Save metrics to CSV\nmetrics_df = pd.DataFrame({\n    'Epoch': range(1, num_epochs + 1),\n    'Val_Precision': val_metrics['precision'],\n    'Val_Recall': val_metrics['recall'],\n    'Val_F1': [precision_recall_fscore_support(all_targets_val, all_preds_val, average='weighted')[2] for _ in range(num_epochs)],\n    'Val_mAP50': val_metrics['mAP50'],\n    'Val_mAP50_95': val_metrics['mAP50_95']\n})\nmetrics_df.to_csv('metrics_results.csv', index=False)\n\nprint(\"Training complete! Results saved as 'training_results.png', 'val_confusion_matrix.png', 'test_confusion_matrix.png', and 'metrics_results.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:40:44.084005Z","iopub.status.idle":"2025-02-21T22:40:44.084595Z","shell.execute_reply.started":"2025-02-21T22:40:44.084149Z","shell.execute_reply":"2025-02-21T22:40:44.084186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'fasterrcnn_model.pth')\nprint(\"Final model saved as 'fasterrcnn_model.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:40:44.085684Z","iopub.status.idle":"2025-02-21T22:40:44.086219Z","shell.execute_reply.started":"2025-02-21T22:40:44.085853Z","shell.execute_reply":"2025-02-21T22:40:44.085893Z"}},"outputs":[],"execution_count":null}]}